\chapter{Introduction}
The convergence of artificial intelligence and creative arts has created infinite opportunities for the creation of digital based artwork. One of the most exciting current uses of deep learning is in Neural Style Transfer (NST), which allows you to take a normal photo and create an amazing piece of art from it, by learning what makes a particular style "art" and then applying that same "art" style to create something new with your own image. This project's goal is to design a seamless style transfer system using CycleGAN (Cycle-Consistent Generative Adversarial Networks) to enable the ability to convert images into 4 very unique artistic styles, which include: One Piece anime, Disney animated characters, Studio Ghibli animated characters, and Van Gogh style painting.\\
The emergence of Generative Adversarial Networks (GANs) in 2014 marked a paradigm shift in generative modeling. Subsequently, the introduction of CycleGAN by Zhu et al. \cite{bib:cyclegan} revolutionized image-to-image translation by eliminating the requirement for paired training data. This advancement is particularly significant for artistic style transfer, where obtaining perfectly aligned pairs of content and stylized images is impractical.\\
This project utilises the characterisation of the above examples to produce an animated human face and to create painting-like landscapes based on this characterisation. Each of these three animation styles (One Piece, Disney, Studio Ghibli) has a unique visual appearance, making it immediately identifiable. The post-impressionist style designed by Vincent Van Gogh is accepted as having a very particular brushstroke and a colour palette that is especially vibrant.\\
\newpage
\section{Background}
\subsection{Evolution of Computer Vision}

Computer vision has transformed dramatically with deep learning. Key milestones include: traditional hand-crafted features (pre-2012), the AlexNet breakthrough (2012) demonstrating CNN superiority, deeper architectures like VGGNet and ResNet (2014-2016), and generative models including GANs (2014-present) enabling image synthesis.

\subsection{Convolutional Neural Networks}

CNNs learn features in a hierarchical manner. The early layers typically capture low-level information such as edges and textures. The intermediate layers build on these to represent more complex patterns, while the deepest layers encode high-level semantic concepts. This hierarchical organisation allows neural style transfer to separate an image's content from its style, enabling generation of new images that preserve the content but adopt an alternative style.VGG-19 is widely used for feature extraction because of its consistent $3\times3$ convolutional architecture and its proven reliability in modelling perceptual similarity.

\subsection{Generative Adversarial Networks}

GANs consist of two components: the generator, which is responsible for generating synthetic samples, and the discriminator, which determines whether or not a given sample is considered real or fake. Using adversarial training techniques, both components are trained until the generator produces samples created in a way that they appear realistic to real humans. Some popular variants of GAN include Conditional GAN, Pix2Pix, Cycle GAN, and Style GAN.
\subsection{Style Transfer Approaches}

GANs comprise two essential elements: a Generator responsible for the creation of synthetic samples, and a Discriminator which judges if a particular sample is classified as real or fake. By using the principles of adversarial training, both components were trained until the generator produces synthetically produced samples in such a manner that they look real to the human eye. Examples of several types of GANs include cGANs (Conditional GANs), Pix2Pix, CycleGANs and StyleGANs.
\section{Motivation}
\vspace{-0.7cm}
The motivation for this project stems from several converging factors in technology, art, and social media culture:\\
\textbf{1. Growing Demand for Personalized Digital Art:}\\
The popularity of photo filter apps and personalized artistic transformation utilities are driven by the need for users to create artistically altered versions of their own images in creative ways. There is a large market demand for systems that can provide high-quality anime or painterly transformations of photographs to fulfill this need.\\
\textbf{2. Bridging Art and Technology:}\\
It normally takes a lot of time and training to develop your artistic skills through traditional means, but thanks to neural style transfer, you do not need either of those to create art. With neural style transfer, anyone can create photo-based artwork that captures the essence of famous artists or animation studios. As a result, neural style transfer has made it easier than ever for people to appreciate and create art.\\
\textbf{3. Advances in Deep Learning:}\\
Deep learning has made tremendous strides in just a few years. Developments in image generation and transformation based on GANs (Generative Adversarial Networks) enable the creation of complex and highly realistic photographic imagery via computational techniques. Combining the development of very capable GPU computing systems with the growing number of open-source frameworks such as PyTorch means that building these capabilities is now easier than ever before.\\
\textbf{4. Cultural Significance of Animation Styles:}\\
Japanese Animation (One Piece) is an entirely different visual vocabulary than Western Animation (Disney) and Studio Ghibli. Fans of both styles have a huge following around the world and this is why there is such a high demand to develop software that converts photographs into these two styles for fans and content developers around the globe.\\
\textbf{5. Preservation and Appreciation of Artistic Heritage:}\\
Considered one of the greatest achievements in post-impressionistic art, Van Gogh's distinctive style features swirling brush strokes and bright colours. It can be used as an educational tool and a way to appreciate the artistic heritage of Van Gogh's work and the influence it has had on contemporary visual art through the application of this style by a neural network trained to apply Van Gogh's style to current day photographs.\\
\textbf{6. Research and Educational Value:}\\
Completing the process of building a full CycleGAN for style transfer was an instructive way to learn about the many areas of designing a Deep Learning Architecture, developing a Loss Function, creating a Dataset, and exploring the difficulties associated with generative models. Thus, this project serves as a thorough example of how to use Applied Deep Learning.
\section{Problem Statement}
The problem addressed by this project can be formally stated as follows:\\
Taking an input image $x$ from domain $X$ (real photographs), the goal is to learn a mapping function $G: X \rightarrow Y$ that transforms $x$ into an output image $G(x)$ in domain $Y$ (stylized images), such that:
\begin{enumerate}
    \item The output $G(x)$ exhibits the visual characteristics of the target artistic style
    \item The semantic content and structure of the input image are preserved
    \item The transformation is realistic and free from artifacts
    \item The system operates efficiently for real-time applications
\end{enumerate}
The specific style domains addressed are:
\begin{itemize}
    \item \textbf{One Piece Style:} Bold outlines, exaggerated expressions, vibrant colors characteristic of Eiichiro Oda's artwork
    \item \textbf{Disney Style:} Smooth gradients, large expressive eyes, soft color palettes of modern Disney animation
    \item \textbf{Studio Ghibli Style:} Watercolor textures, naturalistic expressions, warm earthy tones of Hayao Miyazaki's films
    \item \textbf{Van Gogh Style:} Swirling brushstrokes, impasto technique, vibrant post-impressionist colors
\end{itemize}
\newpage
\section{Objective of the Project}
The primary objectives of this project are:\\
\textbf{1. Develop Style-Specific Neural Style Transfer Models:}\\
You will create four different models for transforming images through CycleGAN methods, each trained on one specific art stimulus. Each model must be capable of reproducing the specific characteristics associated with the targeted artistic styles while preserving the content of the original photograph.\\
\textbf{2. Create Custom Datasets through Frame Extraction:}\\
Develop an automated pipeline for extracting character frames from animation sources (One Piece episodes, Disney movies, Studio Ghibli films) using computer vision techniques including face detection and quality filtering.\\
\textbf{3. Implement and Optimize Core Algorithms:}\\
Implement the following key algorithms with improvements tailored to our application:
\begin{itemize}
    \item CycleGAN architecture with ResNet-based generators
    \item PatchGAN discriminators for local style assessment
    \item Cycle-consistency loss for bidirectional mapping
    \item Identity loss for color preservation
    \item Perceptual loss using VGG-19 features
    \item Custom frame extraction algorithm
\end{itemize}
\textbf{4. Optimize for Real-Time Performance:}\\
Achieve inference times of under 3 seconds per image through model optimization, efficient data pipelines, and GPU acceleration.\\
\textbf{5. Build a Modular and Extensible System:}\\
Design the system architecture to allow easy addition of new styles and modification of existing models.\\
\newpage
\section{Scope of the Project}
The scope of this project encompasses the following:\\
\textbf{Included in Scope:}
\begin{itemize}
    \item Implementation of CycleGAN-based style transfer for four artistic styles
    \item Custom dataset creation through frame extraction from video sources
    \item Training pipeline with configurable hyperparameters
    \item Inference system for single image transformation
    \item Documentation and analysis of results
    \item Comparison with existing methods
\end{itemize}
\newpage
\section{Methodology Overview}
The methodology employed in this project follows a systematic six-phase approach, as illustrated in Figure \ref{fig:methodology}. The pipeline begins with dataset collection and frame extraction from animation sources and public datasets, followed by data preprocessing and augmentation to prepare images for training. The CycleGAN model design phase implements the generator and discriminator architectures, which are then trained using multiple loss functions. Finally, evaluation and optimization assess model performance before deploying the inference system for real-world use.

\usetikzlibrary{positioning}
\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm, % more vertical spacing
    block/.style={
        rectangle,
        draw,
        fill=blue!20,
        text width=10em,
        align=center,
        rounded corners,
        minimum height=3em
    },
    line/.style={->, thick, >=stealth', shorten >=3pt, shorten <=3pt}
 % arrow style
]

    \node[block] (dataset) {Dataset Collection\\\& Frame Extraction};
    \node[block, below=of dataset] (preprocess) {Data Preprocessing\\\& Augmentation};
    \node[block, below=of preprocess] (model) {CycleGAN\\Model Design};
    \node[block, below=of model] (train) {Model Training\\with Multiple Loss Functions};
    \node[block, below=of train] (eval) {Evaluation\\\& Optimization};
    \node[block, below=of eval] (deploy) {Inference System\\Deployment};

    % arrows from edge to edge
    \draw[line] (dataset.south) -- (preprocess.north);
    \draw[line] (preprocess.south) -- (model.north);
    \draw[line] (model.south) -- (train.north);
    \draw[line] (train.south) -- (eval.north);
    \draw[line] (eval.south) -- (deploy.north);

\end{tikzpicture}
\caption{Project Methodology Overview}
\label{fig:methodology}
\end{figure}
\newpage
\textbf{Phase 1: Dataset Collection and Preparation}
\vspace{-0.2cm}
\begin{itemize}
    \item Extract character frames from One Piece episodes, Disney animated movies, and Studio Ghibli films
    \vspace{-0.2cm}
    \item Collect Van Gogh paintings and landscape photographs from Kaggle
    \vspace{-0.2cm}
    \item Collect human face photographs from Kaggle datasets
\end{itemize}
\textbf{Phase 2: Data Preprocessing}
\vspace{-0.5cm}
\begin{itemize}
    \item Resize all images to 256$\times$256 pixels
    \vspace{-0.5cm}
    \item Apply data augmentation (horizontal flip, rotation)
    \vspace{-0.5cm}
    \item Normalize pixel values to [-1, 1] range
    \vspace{-0.5cm}
    \item Create training and validation splits
\end{itemize}
\textbf{Phase 3: Model Implementation}
\vspace{-0.5cm}
\begin{itemize}
    \item Implement generator networks with ResNet architecture
    \vspace{-0.5cm}
    \item Implement PatchGAN discriminator networks
    \vspace{-0.5cm}
    \item Define loss functions (adversarial, cycle-consistency, identity, perceptual)
    \vspace{-0.5cm}
    \item Set up training loops with Adam optimizer
\end{itemize}
\textbf{Phase 4: Training and Optimization}
\vspace{-0.5cm}
\begin{itemize}
    \item Train separate models for each style
    \vspace{-0.5cm}
    \item Monitor training progress with validation metrics
    \vspace{-0.5cm}
    \item Apply learning rate scheduling
    \vspace{-0.5cm}
    \item Save best model checkpoints
\end{itemize}

\textbf{Phase 5: Evaluation and Deployment}
\vspace{-0.5cm}
\begin{itemize}
    \item Evaluate models using FID scores and visual inspection
    \vspace{-0.5cm}
    \item Optimize inference pipeline for speed
    \vspace{-0.5cm}
    \item Document results and create demonstration system
\end{itemize}

\section{Organisation of the Report}
This project report is organized into the following chapters:\\
\textbf{Chapter 1: Introduction}\\
An overview of the project is provided, including motivation, problem statement, project objectives, scope and methodology. This chapter outlines the general context and importance of neural style transfer as part of the larger field of deep learning.\\
\textbf{Chapter 2: Literature Survey}\\
An overview of the project is provided, including motivation, problem statement, project objectives, scope and methodology. This chapter outlines the general context and importance of neural style transfer as part of the larger field of deep learning.\\
\newpage
\textbf{Chapter 3: System Overview}\\
The descriptions of the overall system architecture will include the styles transfer pipeline, methodology for creating the data set and the four styles used. This chapter presents an overview of the relationships between the various components of the system at a higher level.\\
\textbf{Chapter 4: System Architecture and High-Level Design}\\
Presents the detailed architecture of the CycleGAN system, including generator and discriminator networks, loss function formulations, and training procedures. UML diagrams, flowcharts, and architectural diagrams illustrate the design.\\
\textbf{Chapter 5: Software Architecture and Low-Level Design}\\
Provides detailed algorithm descriptions, pseudocode, and implementation specifics. This chapter covers the frame extraction algorithm, network architectures, and training procedures at a granular level.\\
\textbf{Chapter 6: Results}\\
Presents the experimental setup, test procedures, and results obtained from training and evaluating the four style transfer models. Includes sample outputs, quantitative metrics, and comparative analysis.\\
\textbf{Chapter 7: Conclusion}\\
Summarizes the project achievements, discusses limitations, and suggests directions for future work.\\
\textbf{Appendices}\\
Contains supplementary material including project timeline, budget estimation, dataset details, SDG alignment, and configuration information.