\chapter{Literature Survey}
This chapter presents a comprehensive review of the existing research in neural style transfer, generative adversarial networks, and image-to-image translation that forms the foundation of our implementation.\\
Gatys et al., \cite{bib:gatys2016} developed the first deep learning method capable of separating image style from content using a pre-trained VGG network. They demonstrated that higher-level feature maps capture the scene's structure, while the Gram matrices of lower layers encode texture and colour information. Their optimization-based approach generates a new image that matches the content representation of one image while adopting the style statistics of another. This enables the creation of artwork that mimics famous artists while preserving the original scene's objects. Although flexible, the method requires significant computation time, establishing it as a foundational reference for subsequent neural style transfer research.\\
Johnson et al., \cite{bib:johnson2016} replaced pixel-wise loss functions with perceptual loss functions based on VGG features to train fast feedforward networks for style transfer. A single network can learn to approximate the expensive optimization enabling real-time stylization at comparable quality. Their experiments show that feature-space losses produce sharper super-resolution results than MSE-based approaches, as human perception correlates better with high-level feature differences. This work significantly influenced architectural design choices for real-time style transfer networks.\\
Ulyanov et al., \cite{bib:instancenorm} demonstrated that replacing batch normalization with instance normalization substantially improves the quality of feed-forward style transfer networks. Instance normalization normalizes each feature map independently, removing instance-specific contrast and allowing the generator to focus on style statistics rather than global illumination. Networks trained with this modification achieve results comparable to optimization-based methods while operating in real-time. This work established instance normalization as a standard component in style transfer and texture synthesis networks.\\
\newpage
Huang and Belongie, \cite{bib:adain} proposed Adaptive Instance Normalization (AdaIN), a technique that aligns the channel-wise mean and variance of content features to match those of the style image. This approach enables arbitrary style transfer without retraining, allowing any style to be applied to any content image while being over 100x faster than optimization-based methods. AdaIN also provides intuitive controls for adjusting the content-style trade-off, interpolating between multiple styles, and applying spatial or colour constraints. This technique has become a foundational component in arbitrary style transfer models and image-to-image translation applications.\\
Li et al., \cite{bib:li2017} proposed a universal style transfer method using VGG as an encoder with cascaded decoders and whitening and colouring transforms (WCT) in feature space. The WCT matches the covariance of content features to that of the style image through a multi-level pipeline, progressing from coarse to fine layers. Since the decoders are trained only for reconstruction, the method generalizes to unseen styles without retraining. This approach bridges Gram-matrix optimization with feed-forward networks through explicit feature covariance matching.\\
Ghiasi et al., \cite{bib:ghiasi2017} extended fast arbitrary style transfer by developing a network that predicts affine parameters for conditional instance normalization layers. Trained on approximately 80,000 paintings and thousands of textures, the model performs real-time style transfer for previously unseen styles. The learned style embeddings enable smooth interpolation between different styles. This work demonstrated that predicting normalization parameters allows feed-forward networks to function as efficient conditional networks for arbitrary styles.\\
Luan et al., \cite{bib:luan2017} focused on transferring visual styles between photographic images rather than paintings. They introduced an additional photorealism loss based on the Matting Laplacian to constrain local colour adjustments and preserve photorealistic appearance. Semantic segmentation masks guide the transfer to ensure corresponding regions are matched appropriately. Their method produces high-quality results across diverse photographic scenes.\\
Zhu et al., \cite{bib:cyclegan} proposed CycleGAN, an unpaired image-to-image translation framework using adversarial and cycle-consistency losses. Two generators learn mappings $X \rightarrow Y$ and $Y \rightarrow X$, while discriminators enforce realism. The cycle-consistency loss ensures $F(G(x)) \approx x$ and $G(F(y)) \approx y$, preventing mode collapse without paired supervision. The method successfully handles diverse tasks including horse-zebra, summer-winter, and photo-painting conversions, establishing a standard baseline for unpaired translation research.\\
Zhang et al., \cite{bib:zhang2023} combined AdaIN with Gram-matrix style features to create a faster and more flexible digital art generation system. Their model improves content and style accuracy by approximately 15\% over existing approaches, achieving an SSIM of 0.88 at medium style intensity while reducing processing time by nearly 76\%. The method handles diverse artistic styles while preserving content structure, making neural style transfer more practical for designers and multimedia creators.\\
Scott et al., \cite{bib:scott2024} reviewed developments combining GAN image synthesis with neural style transfer. Their survey covers traditional VGG-based style transfer, efficient feed-forward networks with AdaIN, and various GAN architectures including StyleGAN. They also examine diffusion-based models and their applications in img2img pipelines and ControlNet workflows. The review discusses how recent models have expanded creative potential while raising concerns about authorship and authenticity. The authors advocate for a symbiotic relationship between humans and AI, where humans provide creative vision while AI assists with variation and execution.\\
Karras et al., \cite{bib:stylegan} proposed StyleGAN, a generator architecture providing unprecedented control over image synthesis. The architecture features a mapping network that transforms latent codes into an intermediate style space, with adaptive instance normalization modulating convolutional layers. This design separates high-level attributes (pose, identity) from stochastic variations (freckles, hair texture), enabling intuitive style mixing at multiple scales. StyleGAN achieves state-of-the-art face generation quality with smooth latent space transitions, significantly influencing subsequent work on controllable image generation and style manipulation.\\
Jing et al., \cite{bib:nst_survey} conducted a comprehensive survey of neural style transfer methods, covering loss functions, network architectures, and evaluation metrics. They examined emerging directions including semantic-aware, video, and 3D stylization techniques. The survey identifies key challenges such as content leakage, style consistency, and computational efficiency. This work serves as an essential reference for understanding the current state and future directions of neural style transfer research.\\
\newpage
Isola et al., \cite{bib:pix2pix} proposed pix2pix, a versatile image-to-image translation framework using conditional GANs for paired training data. The framework combines adversarial loss with L1 reconstruction loss to preserve spatial information. A PatchGAN discriminator classifies local image patches rather than entire images, improving high-frequency detail. The method successfully addresses diverse tasks including semantic segmentation to photo synthesis, edge-to-photo generation, and day-to-night conversion, establishing a powerful paradigm for supervised image translation.