\chapter{Literature Survey}
This chapter presents a comprehensive review of the existing research in neural style transfer, generative adversarial networks, and image-to-image translation that forms the foundation of our implementation.\\
\textbf{In paper \cite{bib:gatys2016}, A Neural Algorithm of Artistic Style:}
Gatys et al. introduce the first deep-learning method that explicitly separates image content and style using a pretrained VGG network. High-level feature maps encode scene structure, while Gram matrices of lower layers capture textures and colors. By optimizing a random image to match the content representation of one image and the style statistics of another, the algorithm synthesizes paintings that mimic famous artists while preserving underlying objects. This optimization-based approach is flexible but computationally expensive, and it became the foundational reference for nearly all later neural style transfer methods.\\
\textbf{In paper \cite{bib:johnson2016}, Perceptual Losses for Real-Time Style Transfer and Super-Resolution:}
Johnson et al. replace pixel-wise training losses with perceptual losses computed on VGG features to train fast feed-forward networks for image transformation. For style transfer, a single network is optimized to approximate the expensive Gatys objective, producing stylized images in real time with comparable visual quality. For single-image super-resolution, using feature-space losses yields sharper, more detailed outputs than MSE-based training. The work shows that high-level feature differences correlate better with human perception, enabling efficient networks that still respect semantic structure and texture, and it strongly influenced later real-time stylization architectures.\\
\textbf{In paper \cite{bib:instancenorm}, Instance Normalization: The Missing Ingredient for Fast Stylization:}
Ulyanov et al. revisit feed-forward style transfer networks and discover that simply replacing batch normalization with instance normalization dramatically improves visual quality. Instance normalization normalizes each feature map per image, effectively discarding instance-specific contrast and making the generator focus on style statistics instead of global illumination. Trained with the standard Gatys losses, the modified architecture produces images comparable to optimization-based methods but at real-time speed. The paper provides both empirical evidence and an intuitive explanation, establishing instance normalization as a standard component in style transfer and texture synthesis networks.\\
\textbf{In paper \cite{bib:adain}, Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization:}
Huang and Belongie propose Adaptive Instance Normalization (AdaIN), a simple layer that aligns the channel-wise mean and variance of content features to those of a style image. A fixed encoder–decoder network, combined with AdaIN, directly produces stylized images for any style without retraining. The method matches the flexibility of optimization-based NST while running nearly three orders of magnitude faster. It also supports intuitive controls such as content–style trade-off, style interpolation, and color or spatial constraints. AdaIN became a core mechanism for many later arbitrary style transfer and guided image-to-image translation models.\\
\textbf{In paper \cite{bib:li2017}, Universal Style Transfer via Feature Transforms:}
Li et al. aim for universal feed-forward style transfer without training on specific styles. They use a VGG encoder and cascaded decoders, inserting whitening and coloring transforms (WCT) in feature space to match the covariance of content features to that of the style image. A multi-level pipeline progressively applies WCT from coarse to fine layers, capturing both global structure and detailed textures. Since only decoders are trained for reconstruction, the method generalizes to unseen styles while retaining good quality and efficiency. It bridges Gram-matrix optimization and feed-forward networks through explicit feature covariance matching.\\
\textbf{In paper \cite{bib:ghiasi2017}, Exploring the Structure of a Real-Time, Arbitrary Stylization Network:}
Ghiasi et al. extend fast arbitrary style transfer by learning a style prediction network that maps a style image to the affine parameters of conditional instance normalization in a style-transfer network. Trained on about 80,000 paintings and thousands of textures, the system performs real-time stylization for previously unseen styles. The learned style embedding enables smooth interpolation between styles and produces results on par with slower optimization methods. This work demonstrates that learning to predict normalization parameters is an effective way to condition feed-forward networks on arbitrary style inputs.\\
\newpage
\textbf{In paper \cite{bib:luan2017}, Deep Photo Style Transfer:}
Luan et al. address the problem of transferring style between photographs rather than paintings. They augment the Gatys loss with a photorealism regularization term based on the Matting Laplacian, which encourages locally affine color transforms that respect edges. A semantic segmentation mask further constrains style transfer to corresponding regions (e.g., sky-to-sky, building-to-building). The method produces convincing photo-style transfers across diverse scenes, addressing a key limitation of painterly NST approaches.\\
\textbf{In paper \cite{bib:cyclegan}, Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks:}
Zhu et al. propose CycleGAN, an unpaired image-to-image translation framework that learns mappings between two visual domains using adversarial and cycle-consistency losses. Generators translate $X \rightarrow Y$ and $Y \rightarrow X$, while discriminators enforce realism. A cycle loss encourages $F(G(x)) \approx x$ and $G(F(y)) \approx y$, preventing mode collapse and enforcing meaningful correspondences despite lacking paired supervision. The method successfully handles tasks like horse $\leftrightarrow$ zebra, summer $\leftrightarrow$ winter, and photo $\leftrightarrow$ painting conversion. CycleGAN became a standard baseline for unpaired translation and inspired many subsequent guided and multi-domain style synthesis methods.\\
\textbf{In paper \cite{bib:zhang2023}, Applying Deep Learning for Style Transfer in Digital Art:}
Zhang et al. integrate AdaIN with Gram-matrix-based style representation to build an efficient system for digital art creation. By combining the strengths of both approaches, the model achieves a 15\% improvement in content and style loss compared to baselines and an SSIM of 0.88 for medium style strength. Processing time drops by approximately 76\%, enabling near-real-time applications. The method supports diverse artistic styles while maintaining structural details, making neural style transfer more practical for designers and multimedia creators.\\
\textbf{In paper \cite{bib:scott2024}, Blending Art and Intelligence: Advances in Neural Style Transfer and Image Synthesis:}
Scott et al. survey recent progress at the intersection of neural style transfer and GAN-based image synthesis. The paper explains classic NST with VGG features, discusses efficiency improvements like feed-forward networks and AdaIN, and reviews GAN architectures such as StyleGAN and its variants. It also covers diffusion-based generative models and their application to artistic workflows (e.g., Deforum), img2img pipelines, and ControlNet-based concept design. Case studies demonstrate how these tools expand creative possibilities while raising questions about authorship, authenticity, and societal impact. The work argues for a symbiotic relationship between artists and AI, where machines act as collaborators handling large-scale variation and labor, while humans retain conceptual direction and "humanness" in the creative process.\\
\textbf{In paper \cite{bib:stylegan}, A Style-Based Generator Architecture for Generative Adversarial Networks:}
Karras et al. introduce StyleGAN, a novel generator architecture that enables unprecedented control over image synthesis. The key innovation is a mapping network that transforms latent codes into an intermediate style space, which then modulates convolutional layers via adaptive instance normalization. This design separates high-level attributes (pose, identity) from stochastic variations (freckles, hair texture) and allows intuitive mixing of styles at different scales. StyleGAN achieves state-of-the-art results on face generation benchmarks and enables smooth interpolation in style space. The architecture profoundly influenced subsequent work in controllable image generation and style-based manipulation.\\
\textbf{In paper \cite{bib:nst_survey}, Neural Style Transfer: A Review:}
Jing et al. provide a comprehensive survey of neural style transfer methods, categorizing approaches into optimization-based, feed-forward, and arbitrary style transfer. The review covers loss functions, network architectures, and evaluation metrics used across the field. It discusses extensions such as semantic-aware stylization, video style transfer, and 3D style transfer. The paper also addresses open challenges including content leak, style consistency, and computational efficiency. This survey serves as an essential reference for understanding the evolution and current state of neural style transfer research.\\
\textbf{In paper \cite{bib:pix2pix}, Image-to-Image Translation with Conditional Adversarial Networks:}
Isola et al. propose Pix2Pix, a general-purpose framework for paired image-to-image translation using conditional GANs. The method combines an adversarial loss with an L1 reconstruction loss, where the generator learns to produce outputs indistinguishable from real images while preserving structure from the input. A PatchGAN discriminator classifies local image patches rather than the entire image, encouraging high-frequency detail. The framework successfully handles diverse tasks including semantic segmentation to photo, edges to objects, and day to night conversion. Pix2Pix established conditional adversarial training as a powerful paradigm for supervised image translation tasks.

