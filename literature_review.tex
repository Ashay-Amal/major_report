\chapter{Literature Survey}
This chapter presents a comprehensive review of the existing research in neural style transfer, generative adversarial networks, and image-to-image translation that forms the foundation of our implementation.\\
\textbf{In paper \cite{bib:gatys2016}, A Neural Algorithm of Artistic Style:}
Using a already trained VGG network, Gatys et al have developed the very first deep learning method capable of separating both the image style and content. The authors claim that the higher level feature maps contain the scene's structure, whereas the lower layers Gram matrix contain the texture and color information of an image. The authors propose an optimization method where a random image is created so that it matches the content representation of a given image, while also matching the style statistics of another image. The resulting output from this process was the ability to create new paintings that mimic famous artists while preserving the underlying objects within those works of art. While this method was flexible in its execution, it required an extensive amount of time and computational resources to produce results, thus making it a foundational reference for most neural style transfer methods that have developed since then.\\
\textbf{In paper \cite{bib:johnson2016}, Perceptual Losses for Real-Time Style Transfer and Super-Resolution:}
Johnson and others replace the pixel-by-pixel loss functions used to train slow convolutional networks with perceptual loss functions based on VGG model features. They use this new type of loss function to train fast feedforward networks for image-to-image mapping tasks such as style transfer. Specifically, a single neural network can learn how to mimic the expensive losses associated with Gatys et al.’s style transfer approach, allowing the generation of stylized images in real-time, generating them at similar levels of quality as Gatys’s original loss functions. They demonstrate that by using feature-space loss functions, networks trained to produce super-resolved images from single images produce sharper, more detailed results than those trained using MSE-based approaches. They conclude that human perception better correlates with high-level differences in features, which allow for training networks that are efficient yet maintain the same important semantic and textural properties as original images, thus having a strong influence on future architectural design choices for real-time style transfer networks.\\
\textbf{In paper \cite{bib:instancenorm}, Instance Normalization: The Missing Ingredient for Fast Stylization:}
In a study conducted by Ulyanov et al., the authors revisited feed-forward style transfer networks with the aid of new data which showed that replacing batch normalization with instance normalization drastically increased the quality of images produced using these networks. Using instance normalization allowed for normalization of the entire feature map of an input image, eliminating any contrast related to individual instances; therefore, the generator focused on the statistical nature of styles rather than global illumination. The results obtained from training the feed-forward network with the standard loss functions of Gatys compared favourably with the results obtained from the conventional optimization techniques and could be achieved in real-time. The findings of this study fulfilled two objectives: (1) Provided empirical support for the proposed use of instance normalization as a standard component in all style transfer and texture synthesis networks and (2) Provided an intuitive explanation as to why this standardisation of instance-normalised output would yield improved results.\\
\textbf{In paper \cite{bib:adain}, Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization:}
A new approach for performing arbitrary style-transfer via neural networks has been proposed by Huang and Belongie. They refer to the technique as Adaptive Instance Normalization (AdaIN) and describe it as a layer in a neural network that normalizes the mean and variance of features from the content image, to match that of the style image based on channel. The model does not require retraining, this is advantageous given that it allows for any potential combination of style to be applied to any content image. The authors note that their technique represents similar flexibility as traditional optimization techniques used in style-transfer, while being over 100x faster. The benefits of their technique also include intuitive controls such as: a trade-off between content and style; interpolating between two or more styles; and spatial or color-based constraints. AdaIN has also been introduced as a foundational building block for later stages in arbitrary style-transfer models, and for guided image retrieval and image-to-image translation applications.\\
\newpage
\textbf{In paper \cite{bib:li2017}, Universal Style Transfer via Feature Transforms:}
To perform universal feed-forward style transfer without requiring any training specifically for the styles used in feed-forward style transfer, Li et al. designed their system using VGG as an encoder and cascaded decoders along with white and color transforms (WCTs) applied in the feature space which matches the covariance of the content features with that of the style image. Through a multi-level pipeline, the WCT is applied in the model, moving progressively from coarse layers to fine layers. The final product retains the global structure and detailed texture information from both the coarse and fine layers. Because all models are trained only for reconstruction on the decoders, this allows the method to be applied to new styles previously unseen with reasonable quality and efficiency. Finally, the authors bridge Gram-matrix optimization with feed-forward networks through a matching of explicit feature covariances.\\
\textbf{In paper \cite{bib:ghiasi2017}, Exploring the Structure of a Real-Time, Arbitrary Stylization Network:}
The authors Ghiasi et Al. have extended the fast arbitrary style transfer to work by developing a network that predicts the style of an image based upon the affine parameterisation of the conditional instance normalisation layer(s) in a style transfer network. They trained their model with around 80,000 paintings and thousands of different textures. It is capable of real-time style transfers between styles not previously seen. The style embeddings learned during training allow for smooth transition between different styles and speed the performance of style transfer networks. This study has shown that the ability to predict normalisation parameters may allow those feed-forward networks to perform efficiently as conditional networks for arbitrary input styles.\\
\textbf{In paper \cite{bib:luan2017}, Deep Photo Style Transfer:}
Unlike Gatys et al. (2015), the authors of this work focus on transferring visual styles between photographic images instead of painting images. They create an additional loss in their neural network models from the Gatys et al. (2015) work. This added loss incorporates the Matting Laplacian method for photorealism to restrict the amount of local colour adjustments allowed in their datasets. They use semantic segmentation masks to limit the regions being transferred to match the original. The authors show how effective their techniques are through their ability to produce high-quality results from multiple different types of photographic scenes.\\
\newpage
\textbf{In paper \cite{bib:cyclegan}, Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks:}
Zhu et al. propose CycleGAN, an unpaired image-to-image translation framework that learns mappings between two visual domains using adversarial and cycle-consistency losses. Generators translate $X \rightarrow Y$ and $Y \rightarrow X$, while discriminators enforce realism. A cycle loss encourages $F(G(x)) \approx x$ and $G(F(y)) \approx y$, preventing mode collapse and enforcing meaningful correspondences despite lacking paired supervision. The method successfully handles tasks like horse $\leftrightarrow$ zebra, summer $\leftrightarrow$ winter, and photo $\leftrightarrow$ painting conversion. CycleGAN became a standard baseline for unpaired translation and inspired many subsequent guided and multi-domain style synthesis methods.\\
\textbf{In paper \cite{bib:zhang2023}, Applying Deep Learning for Style Transfer in Digital Art:}
Zhang et al. combine AdaIN with Gram-matrix style features to create a faster and more flexible system for digital art generation. By leveraging the strengths of both methods, their model improves content and style accuracy by about 15\% over existing approaches and reaches an SSIM of 0.88 at medium style intensity. It also cuts processing time by nearly 76\%, making the system feel almost real-time. Overall, the method handles a wide variety of artistic styles while still preserving the structure of the original image, making neural style transfer far more practical for designers and multimedia creators.\\
\textbf{In paper \cite{bib:scott2024}, Blending Art and Intelligence: Advances in Neural Style Transfer and Image Synthesis:}
In their review of developments combining GAN image synthesis with neural style transfer, Scott and co-authors present the benefits of each method and how they can be utilised together to enhance creative expression. The review describes traditional neural style transfer using VGG features (and variations of VGG), identifies the efficiency of using feed-forward networks and AdaIN to develop faster versions of neural style transfer processes, and explores many GAN architectures including StyleGAN and its different implementations. Another important aspect to consider is diffusion-based models for generating images and related applications within multi-step workflows or concept designs (e.g., Deforum) and img2img pipelines, as well as the development of workflows and/or workflows built on the ControlNet architecture. Additionally, case studies illustrate how many of the most recent models have broadened creative potential, as well as raised concerns over authorship, authenticity, and society's role in using AI technology to create art. Finally, Scott and co-authors ultimately argue for the need for a "symbiotic relationship" between humans and AI, where humans provide the vision and mission for creativity, and AI technology serves to process large amounts of variation and lend support in accomplishing this mission.\\
\textbf{In paper \cite{bib:stylegan}, A Style-Based Generator Architecture for Generative Adversarial Networks:}
Karras et al. proposed a new generator architecture referred to as StyleGAN, which provides groundbreaking control over the generation/synthesis of images. The new aspect of StyleGAN is a mapping network that creates (maps) a latent code into an intermediary "style" space, and a mechanism that uses adaptive instance normalization to allow the mapping to modulate/modify the convolutional layers. The distinction between the high-level features of an image (e.g., pose, identity) and the stochastic features (e.g., freckles, hair texture) allows for the mixing of styles at various scales very intuitively. Additionally, StyleGAN achieves state-of-the-art performance on face generation tasks, produces smooth transitions in style space, and has had a significant impact on the development of subsequent methods for both controllable image generation and style manipulation.\\
\textbf{In paper \cite{bib:nst_survey}, Neural Style Transfer: A Review:}
Neuroscience researchers Jing and their associates conducted a comprehensive survey that included the types of work that have been done to develop a better understanding of neural style transfer and how it can be utilized in different contexts. They also addressed the many losses that have been identified due to neural style transfer work as well as the network architectures, evaluation metrics, etc., that have been used in the field of style transfer. In addition, they offered their insights into new types of stylization such as semantic awareness, video, and 3D stylization methods. Finally, they highlighted several challenges that remain in the area of neural style transfer, including content leaking, maintaining style consistency, and providing efficiency through computation. As such, this survey serves to be a monumental reference point for understanding how and where the research in the area of neural style transfer is heading and expanding at present.\\
\textbf{In paper \cite{bib:pix2pix}, Image-to-Image Translation with Conditional Adversarial Networks:}
According to Isola and others (2016), pix2pix is a versatile image-to-image translation framework that allows for paired images to be transformed into one another using conditional GANs. In this framework, the GAN generator loses its outputs when compared to the original images (adversarial loss), while also retaining spatial information from the input image through a L1 loss. The PatchGAN discriminator uses local patches of the generated and real images when classifying (instead of classifying entire images) to allow for more detail at the high frequency end of the scale. The pix2pix framework has been used successfully to address many different types of problems, ranging from semantic segmentation to photographs of objects and edge maps to photographs of objects, as well as problems related to conversion of photographs during the daytime into converted nighttime photographs. The pix2pix framework has become a powerful new area for supervised image translation using conditional adversarial training.

