
\begin{appendices}

\chapter{Project Planning}

\section{Project Timeline}

% PLACEHOLDER: Insert Gantt chart
% Filename: gantt_chart.png
% Description: Gantt chart showing project phases from dataset collection to final testing

\begin{figure}[H]
\centering
\fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textbf{[INSERT: gantt\_chart.png]}\vspace{2cm}}}
\caption{Project Timeline}
\label{fig:timeline}
\end{figure}

\begin{table}[H]
\centering
\caption{Project Milestone Schedule}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Phase} & \textbf{Activity} & \textbf{Duration} \\ \hline
Phase 1 & Literature Review \& Research & 2 weeks \\ \hline
Phase 2 & Dataset Collection \& Preparation & 3 weeks \\ \hline
Phase 3 & Model Architecture Implementation & 5 weeks \\ \hline
Phase 4 & Model Training \& Optimization & 4 weeks \\ \hline
Phase 5 & Testing, Evaluation \& Documentation & 2 weeks \\ \hline
\multicolumn{2}{|l|}{\textbf{Total Duration}} & \textbf{16 weeks} \\ \hline
\end{tabular}
\end{table}

\section{Budget Estimation}

\begin{table}[H]
\centering
\caption{Project Budget Estimation}
\begin{tabular}{|l|l|r|}
\hline
\textbf{Category} & \textbf{Item} & \textbf{Cost (INR)} \\ \hline
\multirow{3}{*}{Hardware} & GPU Cloud Computing (Google Colab Pro) & 2,500/month \\ \cline{2-3}
 & Storage (Cloud) & 500/month \\ \cline{2-3}
 & Miscellaneous Hardware & 2,000 \\ \hline
\multirow{2}{*}{Software} & Dataset (Kaggle - Free) & 0 \\ \cline{2-3}
 & Development Tools (Open Source) & 0 \\ \hline
\multirow{2}{*}{Other} & Documentation \& Printing & 1,500 \\ \cline{2-3}
 & Contingency & 1,500 \\ \hline
\multicolumn{2}{|l|}{\textbf{Total Estimated Cost (4 months)}} & \textbf{17,000} \\ \hline
\end{tabular}
\end{table}

\chapter{Sustainable Development Goals (SDGs) Addressed}

\begin{longtable}{|p{8cm}|c|}
\hline
\textbf{SDG} & \textbf{Level} \\ \hline
No Poverty & 1 \\ \hline
Zero Hunger & 1 \\ \hline
Good Health and Well-being & 1 \\ \hline
Quality Education & 3 \\ \hline
Gender Equality & 2 \\ \hline
Clean Water and Sanitation & 1 \\ \hline
Affordable and Clean Energy & 1 \\ \hline
Decent Work and Economic Growth & 2 \\ \hline
Industry, Innovation and Infrastructure & 3 \\ \hline
Reduced Inequalities & 2 \\ \hline
Sustainable Cities and Communities & 2 \\ \hline
Responsible Consumption and Production & 2 \\ \hline
Climate Action & 1 \\ \hline
Life Below Water & 1 \\ \hline
Life on Land & 1 \\ \hline
Peace, Justice and Strong Institutions & 1 \\ \hline
Partnerships for the Goals & 2 \\ \hline
\end{longtable}
\textbf{Levels:} Poor = 1, Good = 2, Excellent = 3

\textbf{Justification:}
\begin{itemize}
    \item \textbf{Quality Education (Level 3):} The project provides educational value by demonstrating deep learning concepts and can be used as a teaching tool for neural networks and computer vision.
    \item \textbf{Industry, Innovation and Infrastructure (Level 3):} The project contributes to innovation in digital art creation and has applications in the creative industry.
    \item \textbf{Decent Work and Economic Growth (Level 2):} The technology can create new opportunities in digital content creation and entertainment industries.
\end{itemize}

\chapter{Self-Assessment of the Project}

\begin{longtable}{|p{1cm}|p{7cm}|p{5cm}|p{1.5cm}|}
\hline
\textbf{No.} & \textbf{PO and PSO} & \textbf{Contribution from the project} & \textbf{Level} \\ \hline

1 & \textbf{Engineering Knowledge:} Knowledge of mathematics, engineering fundamentals, and engineering specialization to form solutions for complex engineering problems. & Applied deep learning mathematics, loss functions, optimization algorithms & 3 \\ \hline

2 & \textbf{Problem Analysis:} Identify, formulate, review research literature, and analyze complex engineering problems to reach substantiated conclusions. & Conducted literature survey, identified research gaps, formulated solution approach & 3 \\ \hline

3 & \textbf{Design/development of solutions:} Design creative solutions for complex engineering problems. & Designed CycleGAN architecture with custom improvements for style transfer & 3 \\ \hline

4 & \textbf{Conduct investigations of complex problems:} Conduct investigations using research-based knowledge. & Experimented with different architectures, loss functions, and hyperparameters & 3 \\ \hline

5 & \textbf{Modern tool usage:} Create, select, and apply appropriate techniques, resources, and modern engineering \& IT tools. & Used PyTorch, CUDA, OpenCV, and modern deep learning tools & 3 \\ \hline

6 & \textbf{The Engineer and the world:} Analyze and evaluate societal and environmental impacts. & Considered applications in education, entertainment, and creative industries & 2 \\ \hline

7 & \textbf{Ethics:} Apply ethical principles; commit to professional ethics. & Used publicly available datasets, cited all references appropriately & 2 \\ \hline

8 & \textbf{Individual and Team Work:} Function effectively as an individual and as a member in diverse teams. & Collaborated on different aspects: dataset creation, model training, documentation & 3 \\ \hline

9 & \textbf{Communication:} Communicate effectively within the engineering community. & Prepared comprehensive documentation, created visualizations and diagrams & 3 \\ \hline

10 & \textbf{Project Management and Finance:} Apply engineering management principles. & Planned project phases, managed timeline, estimated budget & 2 \\ \hline

11 & \textbf{Life-long Learning:} Recognize the need for independent and life-long learning. & Learned new deep learning techniques, stayed updated with recent research & 3 \\ \hline

12 & \textbf{PSO1 – Computer-based systems development:} Design computer-based systems. & Developed complete neural network system for image transformation & 3 \\ \hline

13 & \textbf{PSO2 – Software development:} Specify, design, and develop applications. & Implemented training and inference pipelines using best practices & 3 \\ \hline

14 & \textbf{PSO3 – Computer communications and Internet applications:} Design network applications. & System can be deployed as web service with REST API & 2 \\ \hline

\end{longtable}
\textbf{Levels:} Poor = 1, Good = 2, Excellent = 3

\chapter{Dataset Details}

Detailed dataset information is provided in Chapter 3 (System Overview). The summary statistics are shown below:

\begin{table}[H]
\centering
\caption{Complete Dataset Statistics}
\begin{tabular}{|l|c|c|c|l|}
\hline
\textbf{Style} & \textbf{Domain X} & \textbf{Domain Y} & \textbf{Total} & \textbf{Source} \\ \hline
One Piece & 3,000+ & 2,500+ & 5,500+ & Kaggle + Episodes \\ \hline
Disney & 3,000+ & 2,000+ & 5,000+ & Kaggle + Movies \\ \hline
Studio Ghibli & 3,000+ & 2,000+ & 5,000+ & Kaggle + Films \\ \hline
Van Gogh & 2,500+ & 400+ & 2,900+ & Kaggle Datasets \\ \hline
\textbf{Total} & \textbf{11,500+} & \textbf{6,900+} & \textbf{18,400+} & -- \\ \hline
\end{tabular}
\end{table}

All images are preprocessed to 256$\times$256 pixels in RGB format with normalization to [-1, 1] range.

\chapter{Configuration and Usage}

\section{Repository Structure}

The source code is organized in a modular structure to facilitate understanding, modification, and extension:

\begin{table}[H]
\centering
\caption{Project Directory Structure}
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Directory/File} & \textbf{Description} \\ \hline
\texttt{models/} & Neural network architecture definitions \\ \hline
\quad \texttt{generator.py} & ResNet-based generator with 9 residual blocks \\ \hline
\quad \texttt{discriminator.py} & PatchGAN discriminator implementation \\ \hline
\quad \texttt{cyclegan.py} & Complete CycleGAN model with training logic \\ \hline
\texttt{utils/} & Utility functions and helper modules \\ \hline
\quad \texttt{dataset.py} & Custom PyTorch Dataset class for unpaired data \\ \hline
\quad \texttt{transforms.py} & Image preprocessing and augmentation \\ \hline
\quad \texttt{losses.py} & Loss function implementations (perceptual, cycle) \\ \hline
\quad \texttt{buffer.py} & Image history buffer for discriminator training \\ \hline
\texttt{train.py} & Training script with argument parsing \\ \hline
\texttt{inference.py} & Inference script for style transfer \\ \hline
\texttt{checkpoints/} & Saved model weights ($\sim$150 MB per style) \\ \hline
\texttt{data/} & Training datasets organized by style \\ \hline
\texttt{requirements.txt} & Python package dependencies \\ \hline
\end{tabular}
\end{table}

\section{Installation Guide}

\subsection{System Requirements}

\begin{table}[H]
\centering
\caption{System Requirements}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Component} & \textbf{Minimum} & \textbf{Recommended} \\ \hline
GPU & NVIDIA GTX 1060 (6GB) & NVIDIA RTX 3080 (10GB) \\ \hline
CPU & Intel i5 / AMD Ryzen 5 & Intel i7 / AMD Ryzen 7 \\ \hline
RAM & 8 GB & 16 GB \\ \hline
Storage & 20 GB & 100 GB (for datasets) \\ \hline
OS & Ubuntu 18.04 / Windows 10 & Ubuntu 20.04 / Windows 11 \\ \hline
\end{tabular}
\end{table}

\subsection{Installation Steps}

\begin{enumerate}
    \item \textbf{Clone Repository:}
\begin{lstlisting}[language=bash]
git clone https://github.com/user/cyclegan-style-transfer.git
cd cyclegan-style-transfer
\end{lstlisting}

    \item \textbf{Create Virtual Environment:}
\begin{lstlisting}[language=bash]
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows
\end{lstlisting}

    \item \textbf{Install Dependencies:}
\begin{lstlisting}[language=bash]
pip install -r requirements.txt
\end{lstlisting}

    \item \textbf{Verify CUDA Installation:}
\begin{lstlisting}[language=python]
import torch
print(torch.cuda.is_available())  # Should print True
print(torch.cuda.get_device_name(0))
\end{lstlisting}
\end{enumerate}

\subsection{Dependencies}

\begin{table}[H]
\centering
\caption{Python Package Dependencies}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Package} & \textbf{Version} & \textbf{Purpose} \\ \hline
torch & $\geq$2.0.0 & Deep learning framework \\ \hline
torchvision & $\geq$0.15.0 & Image processing and pretrained models \\ \hline
numpy & $\geq$1.21.0 & Numerical operations \\ \hline
Pillow & $\geq$9.0.0 & Image loading and saving \\ \hline
opencv-python & $\geq$4.5.0 & Video processing and face detection \\ \hline
tqdm & $\geq$4.60.0 & Progress bar for training \\ \hline
matplotlib & $\geq$3.5.0 & Visualization and plotting \\ \hline
\end{tabular}
\end{table}

\section{Usage Instructions}

\subsection{Inference (Style Transfer)}

To transform an image using a pre-trained model:

\begin{lstlisting}[language=bash]
python inference.py --input path/to/image.jpg \
                    --style onepiece \
                    --output path/to/result.png
\end{lstlisting}

\textbf{Available Styles:}
\begin{itemize}
    \item \texttt{onepiece} - One Piece anime style
    \item \texttt{disney} - Disney animation style
    \item \texttt{ghibli} - Studio Ghibli animation style
    \item \texttt{vangogh} - Van Gogh painting style
\end{itemize}

\textbf{Additional Options:}
\begin{lstlisting}[language=bash]
--gpu 0           # GPU device ID (default: 0)
--size 256        # Output image size (default: 256)
--checkpoint dir  # Custom checkpoint directory
\end{lstlisting}

\subsection{Training a New Model}

To train a new style transfer model:

\begin{lstlisting}[language=bash]
python train.py --dataroot ./data/style_name \
                --name style_name \
                --epochs 200 \
                --batch_size 4 \
                --lr 0.0002
\end{lstlisting}

\textbf{Training Options:}
\begin{table}[H]
\centering
\caption{Training Command Line Arguments}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Argument} & \textbf{Default} & \textbf{Description} \\ \hline
\texttt{--dataroot} & Required & Path to dataset directory \\ \hline
\texttt{--name} & Required & Experiment name for checkpoints \\ \hline
\texttt{--epochs} & 200 & Total training epochs \\ \hline
\texttt{--batch\_size} & 4 & Batch size for training \\ \hline
\texttt{--lr} & 0.0002 & Initial learning rate \\ \hline
\texttt{--lambda\_cyc} & 10.0 & Cycle consistency loss weight \\ \hline
\texttt{--lambda\_id} & 5.0 & Identity loss weight \\ \hline
\texttt{--n\_res} & 9 & Number of residual blocks \\ \hline
\texttt{--save\_freq} & 10 & Checkpoint save frequency (epochs) \\ \hline
\end{tabular}
\end{table}

\subsection{Dataset Preparation}

Organize datasets in the following structure:

\begin{lstlisting}
data/style_name/
    trainA/     # Domain A (e.g., human faces)
        img001.jpg
        img002.jpg
        ...
    trainB/     # Domain B (e.g., anime faces)
        img001.jpg
        img002.jpg
        ...
    testA/      # Test images from Domain A
    testB/      # Test images from Domain B
\end{lstlisting}

\chapter{Key Code Snippets}

This appendix provides essential code implementations for reference.

\section{Generator Network (Condensed)}

The ResNet-based generator uses reflection padding, instance normalization, and 9 residual blocks:

\begin{lstlisting}[language=Python, caption=Generator Core Structure]
class ResnetGenerator(nn.Module):
    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9):
        # Encoder: 3 conv layers (3->64->128->256 channels)
        # Transform: 9 ResnetBlocks (256 channels)
        # Decoder: 2 transposed conv (256->128->64->3)
        # Output: Tanh activation [-1, 1]

class ResnetBlock(nn.Module):
    def forward(self, x):
        return x + self.conv_block(x)  # Skip connection
\end{lstlisting}

\section{PatchGAN Discriminator (Condensed)}

\begin{lstlisting}[language=Python, caption=Discriminator Structure]
class NLayerDiscriminator(nn.Module):
    # 4 conv layers: 3->64->128->256->512->1
    # LeakyReLU(0.2), InstanceNorm (except first layer)
    # Output: 30x30 patch classification map
\end{lstlisting}

\section{Loss Functions}

\begin{lstlisting}[language=Python, caption=CycleGAN Loss Functions]
# GAN Loss (LSGAN): MSE between pred and target (0 or 1)
# Cycle Loss: L1(F(G(x)), x) + L1(G(F(y)), y)
# Identity Loss: L1(G(y), y) + L1(F(x), x)
# Weights: lambda_cyc=10.0, lambda_id=5.0
\end{lstlisting}

\chapter{Mathematical Summary}

\section{Key Equations}

\textbf{Gram Matrix:} $G^l_{ij} = \sum_k F^l_{ik} F^l_{jk}$

\textbf{Style Loss:} $\mathcal{L}_{style}^l = \frac{1}{4N_l^2 M_l^2} \sum_{i,j} (G^l_{ij}(\hat{y}) - G^l_{ij}(s))^2$

\textbf{Cycle-Consistency:} $\mathcal{L}_{cyc} = \mathbb{E}_{x}[||F(G(x)) - x||_1] + \mathbb{E}_{y}[||G(F(y)) - y||_1]$

\textbf{LSGAN Discriminator:} $\min_D \frac{1}{2}\mathbb{E}_{x}[(D(x) - 1)^2] + \frac{1}{2}\mathbb{E}_{z}[D(G(z))^2]$

\textbf{LSGAN Generator:} $\min_G \frac{1}{2}\mathbb{E}_{z}[(D(G(z)) - 1)^2]$

\textbf{Total Loss:} $\mathcal{L}_{total} = \mathcal{L}_{GAN} + 10\mathcal{L}_{cyc} + 5\mathcal{L}_{id} + \mathcal{L}_{perceptual}$

\chapter{Glossary}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Term} & \textbf{Definition} \\ \hline
CycleGAN & Unpaired image-to-image translation using cycle-consistency \\ \hline
Discriminator & Network classifying images as real or generated \\ \hline
GAN & Generative Adversarial Network for image synthesis \\ \hline
Generator & Network producing synthetic images \\ \hline
Gram Matrix & Feature correlation matrix for style representation \\ \hline
Instance Norm & Per-sample normalization for style transfer \\ \hline
LSGAN & Least Squares GAN for stable training \\ \hline
NST & Neural Style Transfer technique \\ \hline
PatchGAN & Discriminator classifying image patches \\ \hline
Perceptual Loss & Loss using pre-trained network features \\ \hline
Residual Block & Network block with skip connection \\ \hline
VGG-19 & 19-layer CNN for perceptual loss \\ \hline
\end{tabular}
\end{table}

\end{appendices}

